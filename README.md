# StudyGPT
ç”¨æ¥è®©åˆå­¦è€…å­¦ä¹ llmçš„é¡¹ç›®ã€‚
# TinyDialogue å¯¹è¯æ¨¡å‹é¡¹ç›®

è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸­æ–‡å¯¹è¯æ¨¡å‹é¡¹ç›®ï¼ŒåŸºäºTransformeræ¶æ„å®ç°ï¼Œæ”¯æŒè®­ç»ƒå’Œäº¤äº’å¼å¯¹è¯åŠŸèƒ½ã€‚

## é¡¹ç›®ç‰¹ç‚¹

ğŸš€ **è½»é‡é«˜æ•ˆ**ï¼šç²¾ç®€Transformeræ¶æ„ï¼Œæ¨¡å‹å¤§å°ä»…~100MB
ğŸ’¬ **ä¸­æ–‡ä¼˜åŒ–**ï¼šé’ˆå¯¹ä¸­æ–‡å¯¹è¯åœºæ™¯å®šåˆ¶åˆ†è¯å™¨å’Œè®­ç»ƒæ•°æ®
ğŸ“ **ç®€å•æ˜“ç”¨**ï¼šæä¾›å®Œæ•´è®­ç»ƒå’Œæ¨ç†ä»£ç ï¼Œæ— éœ€å¤æ‚é…ç½®
ğŸ”§ **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶è§£è€¦ï¼Œæ˜“äºæ‰©å±•å’Œå®šåˆ¶

## æ¨¡å‹ä¿¡æ¯

| ç‰¹æ€§ | æè¿° |
|------|------|
| æ¨¡å‹åç§° | StudyTinyModel |
| æ¶æ„ | è‡ªå®šä¹‰Transformer (4å±‚, 8å¤´æ³¨æ„åŠ›) |
| åµŒå…¥ç»´åº¦ | 512 |
| è¯è¡¨å¤§å° | 10,000 |
| æœ€å¤§åºåˆ—é•¿åº¦ | 256 |
| è®­ç»ƒæ•°æ® | å°é»„é¸¡50ä¸‡å¯¹è¯è¯­æ–™ |
### æ¨¡å‹ä¸‹è½½
[ModelScopeæ¨¡å‹ä¸»é¡µ](https://www.modelscope.cn/models/fenuolian/StudyTinyModel/summary)
**ç”±äºæ˜¯åœ¨å°é»„é¸¡çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶æœªæ¸…æ™°æ•°æ®ï¼Œå› æ­¤ä¼šå‡ºç°å¯¹è¯çŸ­ã€è¯ä¸é›…ã€æ— å‰å› åæœçš„æƒ…å†µ**
## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºPythonç¯å¢ƒï¼ˆ3.12ï¼‰
conda create -n tinydialogue python=3.12
conda activate tinydialogue

# å®‰è£…ä¾èµ–
pip install torch tqdm
```

### äº¤äº’å¼å¯¹è¯

```python
from My import *
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å®‰å…¨åŠ è½½æ¨¡å‹ï¼ˆPyTorch 2.6+å…¼å®¹ï¼‰
model, optimizer, scheduler, start_epoch, config = load_checkpoint(
"data/model_epoch_2.pt", device
)

# åŠ è½½è¯è¡¨
tokenizer = Tokenizer().load_vocab("vocab.txt")

# è¿›å…¥å¯¹è¯æ¨¡å¼
while True:
  user_input = input("\næ‚¨: ")
  if user_input.lower() in ["exit", "quit", "bye"]:
  break

  # ç”Ÿæˆå“åº”
  response = generate_response(
  model=model,
  tokenizer=tokenizer,
  input_text=user_input,
  device=device,
  max_length=100,
  temperature=0.7
  )

  print(f"AI: {response}")
```

### å¯åŠ¨å¯¹è¯
```bash
python test.py
```

## è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹

### æ•°æ®å‡†å¤‡
1. å°†å¯¹è¯æ•°æ®æ•´ç†ä¸º`.conv`æ ¼å¼
2. æ–‡ä»¶æ ¼å¼ç¤ºä¾‹ï¼š
```
M ç”¨æˆ·å¯¹è¯å†…å®¹1
M åŠ©æ‰‹å›å¤å†…å®¹1
M ç”¨æˆ·å¯¹è¯å†…å®¹2
M åŠ©æ‰‹å›å¤å†…å®¹2
E
```

### è®­ç»ƒæ­¥éª¤
```python
# åˆå§‹åŒ–æ•°æ®é›†
conv_dataset = ConvDataset("data/your_data.conv")
conv_dataset.process_file()

# æ„å»ºè¯è¡¨
tokenizer = conv_dataset.build_vocab(max_vocab_size=10000, vocab_file="vocab.txt")

# åˆ›å»ºè®­ç»ƒæ•°æ®é›†
dialogue_dataset = DialogueDataset(conv_dataset, tokenizer, max_length=256)

# é…ç½®è®­ç»ƒå‚æ•°
config = {
"lr": 5e-3,
"weight_decay": 0.01,
"epochs": 10,
"grad_accum_steps": 4,
"use_amp": True,
"max_grad_norm": 1.0,
"save_every": 2,
"assistant_token_id": tokenizer.vocab["<|assistant|>"]
}

# åˆå§‹åŒ–æ¨¡å‹
model = DialogueModel(tokenizer.vocab_size, d_model=512, n_head=8, n_layers=4)

# åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
trainer = TransformerTrainer(model, train_loader, device, config)
trainer.train()
```

### å¯åŠ¨è®­ç»ƒ
```bash
python train.py
```

## æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | æè¿° |
|------|------|
| `My.py` | æ ¸å¿ƒæ¨¡å‹å®ç°ï¼ˆæ¨¡å‹æ¶æ„ã€åˆ†è¯å™¨ã€æ•°æ®é›†ï¼‰ |
| `train.py` | æ¨¡å‹è®­ç»ƒè„šæœ¬ |
| `dialogue.py` | äº¤äº’å¼å¯¹è¯è„šæœ¬ |
| `data/` | è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½• |
| `vocab.txt` | åˆ†è¯å™¨è¯è¡¨æ–‡ä»¶ |

## é«˜çº§é€‰é¡¹

### ç”Ÿæˆå‚æ•°è°ƒä¼˜
```python
response = generate_response(
model=model,
tokenizer=tokenizer,
input_text=user_input,
device=device,
max_length=150,# å¢åŠ ç”Ÿæˆé•¿åº¦
temperature=0.9,# æé«˜åˆ›é€ æ€§
top_k=50,# ä½¿ç”¨top-ké‡‡æ ·
repetition_penalty=1.2 # å‡å°‘é‡å¤
)
```

### æ€§èƒ½ä¼˜åŒ–
```python
# è®­ç»ƒé…ç½®ä¼˜åŒ–
config = {
"use_amp": True,# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
"grad_accum_steps": 8,# å°æ˜¾å­˜è®¾å¤‡å¢å¤§æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
"batch_size": 16# æ ¹æ®GPUè°ƒæ•´æ‰¹æ¬¡å¤§å°
}
```

## å¸¸è§é—®é¢˜

### æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆPyTorch 2.6+ï¼‰
```python
# æ·»åŠ å®‰å…¨å£°æ˜
from My import DialogueModel
import torch.serialization

torch.serialization.add_safe_globals([DialogueModel])
```

### æ˜¾å­˜ä¸è¶³
1. å‡å°`batch_size`
2. å¢åŠ `grad_accum_steps`
3. å¯ç”¨`use_amp`æ··åˆç²¾åº¦

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼š
- æŠ¥å‘Šé—®é¢˜
- æ·»åŠ æ–°åŠŸèƒ½
- ä¼˜åŒ–æ–‡æ¡£
- æ”¹è¿›æ¨¡å‹æ¶æ„
